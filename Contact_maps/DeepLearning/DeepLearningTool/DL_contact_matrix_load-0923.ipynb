{
 "metadata": {
  "name": "",
  "signature": "sha256:6af8b42e709b43dbf8a970ee06a06ba0417e49c98bc57cb0c82743c04403d69e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "sys.path.append('../../../libs/')\n",
      "import os.path\n",
      "import IO_class\n",
      "from IO_class import FileOperator\n",
      "from sklearn import cross_validation\n",
      "import sklearn\n",
      "import numpy as np\n",
      "import csv\n",
      "from dateutil import parser\n",
      "from datetime import timedelta\n",
      "from sklearn import svm\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pdb\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import preprocessing\n",
      "import sklearn\n",
      "import scipy.stats as ss\n",
      "from sklearn.svm import LinearSVC\n",
      "import random\n",
      "from DL_libs import *\n",
      "from itertools import izip #new\n",
      "import pprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .qhull import *\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .qhull import *\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filename = 'SUCCESS_log_CrossValidation_load_DL_remoteFisherM1_DL_RE_US_DL_RE_US_1_1_19MAY2014.txt'\n",
      "filename = 'listOfDDIsHaveOver2InterfacesHave40-75_Examples_2010_real_selected.txt' #for testing\n",
      "#filename = 'list_of_SUCCESS_log_run3DID_STAT_200_275_examples_201001MAY2014.txt'\n",
      "file_obj = FileOperator(filename)\n",
      "ddis = file_obj.readStripLines()\n",
      "import logging\n",
      "import time\n",
      "current = time.strftime(\"%m_%d_%Y\")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "logname = 'DL_contact_matrix_' + current + '.log'\n",
      "handler = logging.FileHandler(logname)\n",
      "handler.setLevel(logging.DEBUG)\n",
      "\n",
      "# create a logging format\n",
      "\n",
      "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
      "handler.setFormatter(formatter)\n",
      "\n",
      "# add the handlers to the logger\n",
      "\n",
      "logger.addHandler(handler)\n",
      "\n",
      "logger.info('Input DDI file: ' + filename)\n",
      "#logger.debug('This message should go to the log file')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:__main__:Input DDI file: listOfDDIsHaveOver2InterfacesHave40-75_Examples_2010_real_selected.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of lines in listOfDDIsHaveOver2InterfacesHave40-75_Examples_2010_real_selected.txt:37\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ddi  = ddis[0]\n",
      "ddis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "['6PGD_int_NAD_binding_2',\n",
        " 'Activin_recp_int_TGF_beta',\n",
        " 'ADSL_C_int_Lyase_1',\n",
        " 'AICARFT_IMPCHas_int_MGS',\n",
        " 'AIRS_int_AIRS_C',\n",
        " 'Ald_Xan_dh_C_int_FAD_binding_5',\n",
        " 'Alpha-amylase_int_CBM_48',\n",
        " 'AMNp_N_int_PNP_UDP_1',\n",
        " 'ARPC4_int_WD40',\n",
        " 'CagX_int_TrbI',\n",
        " 'Cation_ATPase_C_int_E1-E2_ATPase',\n",
        " 'Ca_chan_IQ_int_efhand',\n",
        " 'CBM_20_int_Glyco_hydro_14',\n",
        " 'Cytochrom_B_C_int_Rieske',\n",
        " 'Cytochrom_B_N_int_UCR_14kD',\n",
        " 'Cytochrom_B_N_int_UCR_Fe-S_N',\n",
        " 'Dioxygenase_C_int_Dioxygenase_N',\n",
        " 'E1-E2_ATPase_int_Hydrolase',\n",
        " 'EFG_C_int_GTP_EFTU',\n",
        " 'efhand_int_IQ',\n",
        " 'efhand_int_Troponin',\n",
        " 'Fapy_DNA_glyco_int_H2TH',\n",
        " 'Fer4_NifH_int_Oxidored_nitro',\n",
        " 'FGF_int_I-set',\n",
        " 'FumaraseC_C_int_Lyase_1',\n",
        " 'Furin-like_int_Recep_L_domain',\n",
        " 'Glyco_hydro_10_int_Ricin_B_lectin',\n",
        " 'GP120_int_ig',\n",
        " 'H2TH_int_zf-FPG_IleRS',\n",
        " 'Ion_trans_2_int_V-set',\n",
        " 'JmjC_int_JmjN',\n",
        " 'Kringle_int_PAN_1',\n",
        " 'MDH_int_PQQ',\n",
        " 'PA_int_Peptidase_M28',\n",
        " 'Peptidase_M28_int_TFR_dimer',\n",
        " 'Photo_RC_int_PSII',\n",
        " 'Stathmin_int_Tubulin']"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DDI_family_base(object):\n",
      "    def __init__(self, ddi, Vectors_Fishers_aaIndex_raw_folder = '/home/du/Documents/Vectors_Fishers_aaIndex_raw_2014/'):\n",
      "    #def __init__(self, ddi, Vectors_Fishers_aaIndex_raw_folder = '/home/sun/Downloads/contactmatrix/contactmatrixanddeeplearningcode/data_test/'):\n",
      "        \"\"\" get total number of sequences in a ddi familgy\n",
      "        Attributes:\n",
      "            ddi: string ddi name\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string, folder\n",
      "            total_number_of_sequences: int\n",
      "            raw_data: dict raw_data[2]\n",
      "        LOO_data['FisherM1'][1]\n",
      "\n",
      "        \"\"\"\n",
      "        self.ddi = ddi\n",
      "        self.Vectors_Fishers_aaIndex_raw_folder = Vectors_Fishers_aaIndex_raw_folder\n",
      "        self.ddi_folder = self.Vectors_Fishers_aaIndex_raw_folder + ddi + '/'\n",
      "        self.total_number_of_sequences = self.get_total_number_of_sequences()\n",
      "        self.raw_data = {}\n",
      "        self.positve_negative_number = {}\n",
      "        self.equal_size_data = {}\n",
      "        for seq_no in range(1, self.total_number_of_sequences+1):\n",
      "            self.raw_data[seq_no] = self.get_raw_data_for_selected_seq(seq_no)\n",
      "            try:\n",
      "                #positive_file = self.ddi_folder + 'numPos_'+ str(seq_no) + '.txt'\n",
      "                #file_obj = FileOperator(positive_file)\n",
      "                #lines = file_obj.readStripLines()\n",
      "                #import pdb; pdb.set_trace()\n",
      "                count_pos = int(np.sum(self.raw_data[seq_no][:, -1]))\n",
      "                count_neg = self.raw_data[seq_no].shape[0] - count_pos\n",
      "                #self.positve_negative_number[seq_no] = {'numPos': int(float(lines[0]))}\n",
      "                #assert int(float(lines[0])) == count_pos\n",
      "                self.positve_negative_number[seq_no] = {'numPos': count_pos}\n",
      "                #negative_file = self.ddi_folder + 'numNeg_'+ str(seq_no) + '.txt'\n",
      "                #file_obj = FileOperator(negative_file)\n",
      "                #lines = file_obj.readStripLines()\n",
      "                #self.positve_negative_number[seq_no]['numNeg'] =  int(float(lines[0]))\n",
      "                self.positve_negative_number[seq_no]['numNeg'] =  count_neg\n",
      "            except Exception,e:\n",
      "                print ddi, seq_no\n",
      "                print str(e)\n",
      "            # get data for equal positive and negative\n",
      "            n_pos = self.positve_negative_number[seq_no]['numPos']\n",
      "            n_neg = self.positve_negative_number[seq_no]['numNeg']\n",
      "            index_neg = range(n_pos, n_pos + n_neg)\n",
      "            random.shuffle(index_neg)\n",
      "            index_neg = index_neg[: n_pos]\n",
      "            positive_examples = self.raw_data[seq_no][ : n_pos, :]\n",
      "            negative_examples = self.raw_data[seq_no][index_neg, :]\n",
      "            self.equal_size_data[seq_no] = np.vstack((positive_examples, negative_examples))\n",
      "    def get_LOO_training_and_reduced_traing(self, seq_no, fisher_mode = 'FisherM1ONLY' , reduce_ratio = 4):\n",
      "        \"\"\" get the leave one out traing data, reduced traing\n",
      "        Parameters:\n",
      "            seq_no: \n",
      "            fisher_mode: default 'FisherM1ONLY'\n",
      "        Returns:\n",
      "            (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced),  (test_X, test_y)\n",
      "        \"\"\"\n",
      "        train_X_LOO = np.array([])\n",
      "        train_y_LOO = np.array([])\n",
      "        train_X_reduced = np.array([])\n",
      "        train_y_reduced = np.array([])\n",
      "\n",
      "        total_number_of_sequences = self.total_number_of_sequences\n",
      "        equal_size_data_selected_sequence = self.equal_size_data[seq_no]\n",
      "\n",
      "        #get test data for selected sequence\n",
      "        test_X, test_y = self.select_X_y(equal_size_data_selected_sequence, fisher_mode = fisher_mode)\n",
      "        total_sequences = range(1, total_number_of_sequences+1)\n",
      "        loo_sequences = [i for i in total_sequences if i != seq_no]\n",
      "        number_of_reduced = len(loo_sequences)/reduce_ratio if len(loo_sequences)/reduce_ratio !=0 else 1\n",
      "        random.shuffle(loo_sequences)\n",
      "        reduced_sequences = loo_sequences[:number_of_reduced]\n",
      "\n",
      "        #for loo data\n",
      "        for current_no in loo_sequences:\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if train_X_LOO.ndim ==1:\n",
      "                train_X_LOO = current_X\n",
      "            else:\n",
      "                train_X_LOO = np.vstack((train_X_LOO, current_X))\n",
      "            train_y_LOO = np.concatenate((train_y_LOO, current_y))\n",
      "\n",
      "        #for reduced data\n",
      "        for current_no in reduced_sequences:\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if train_X_reduced.ndim ==1:\n",
      "                train_X_reduced = current_X\n",
      "            else:\n",
      "                train_X_reduced = np.vstack((train_X_reduced, current_X))\n",
      "            train_y_reduced = np.concatenate((train_y_reduced, current_y))                \n",
      "\n",
      "        return (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y)\n",
      "    def get_total_number_of_sequences(self):\n",
      "        \"\"\" get total number of sequences in a ddi familgy\n",
      "        Parameters:\n",
      "            ddi: string\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string\n",
      "        Returns:\n",
      "            n: int\n",
      "        \"\"\"\n",
      "        folder_path = self.Vectors_Fishers_aaIndex_raw_folder + self.ddi + '/' \n",
      "        filename = folder_path +'allPairs.txt'\n",
      "        all_pairs = np.loadtxt(filename)\n",
      "        return len(all_pairs)\n",
      "\n",
      "    def get_raw_data_for_selected_seq(self, seq_no):\n",
      "        \"\"\" get raw data for selected seq no in a family\n",
      "        Parameters:\n",
      "            ddi: \n",
      "            seq_no: \n",
      "        Returns:\n",
      "            data: raw data in the sequence file\n",
      "        \"\"\"\n",
      "        folder_path = self.Vectors_Fishers_aaIndex_raw_folder + self.ddi + '/' \n",
      "        filename = folder_path + 'F0_20_F1_20_Sliding_17_11_F0_20_F1_20_Sliding_17_11_ouput_'+ str(seq_no) + '.txt'\n",
      "        data = np.loadtxt(filename)\n",
      "        return data\n",
      "    def select_X_y(self, data, fisher_mode = ''):\n",
      "        \"\"\" select subset from the raw input data set\n",
      "        Parameters:\n",
      "            data: data from matlab txt file\n",
      "            fisher_mode: subset base on this Fisher of AAONLY...\n",
      "        Returns:\n",
      "            selected X,  y\n",
      "        \"\"\"\n",
      "        y = data[:,-1] # get lable\n",
      "        if fisher_mode == 'FisherM1': # fisher m1 plus AA index\n",
      "            a = data[:, 20:227]\n",
      "            b = data[:, 247:454]\n",
      "            X = np.hstack((a,b))\n",
      "        elif fisher_mode == 'FisherM1ONLY': \n",
      "            a = data[:, 20:40]\n",
      "            b = data[:, 247:267]\n",
      "            X = np.hstack((a,b))\n",
      "        elif fisher_mode == 'AAONLY':\n",
      "            a = data[:, 40:227]\n",
      "            b = data[:, 267:454]\n",
      "            X = np.hstack((a,b))\n",
      "        else:\n",
      "            raise('there is an error in mode')\n",
      "        return X, y\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import sklearn.preprocessing\n",
      "class Precessing_Scaler_0_9(sklearn.preprocessing.StandardScaler):\n",
      "    def __init__(self):\n",
      "        super(Precessing_Scaler_0_9, self).__init__(self, with_std=0.333)\n",
      "    def transform(self, X): # transform data to 0.1 to 0.9\n",
      "        new_X = super(Precessing_Scaler_0_9, self).transform(X)\n",
      "        print \n",
      "        new_X[new_X > 1] = 1\n",
      "        new_X[new_X < -1] = -1\n",
      "        new_X = (new_X + 1) * 0.4 + 0.1\n",
      "        return new_X\n",
      "    def fit_transform(self):\n",
      "        print 'Did not implement'\n",
      "def performance_score(target_label, predicted_label, predicted_score = False, print_report = True): \n",
      "    \"\"\" get performance matrix for prediction\n",
      "        Attributes:\n",
      "            target_label: int 0, 1\n",
      "            predicted_label: 0, 1 or ranking\n",
      "            predicted_score: bool if False, predicted_label is from 0, 1. If Ture, predicted_label is ranked, need to get AUC score.\n",
      "            print_report: if True, print the perfromannce on screen\n",
      "    \"\"\"\n",
      "    import sklearn\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "    score = {}\n",
      "    if predicted_score == False:\n",
      "        score['accuracy'] = sklearn.metrics.accuracy_score(target_label, predicted_label)\n",
      "        score['precision'] = sklearn.metrics.precision_score(target_label, predicted_label, pos_label=1)\n",
      "        score['recall'] = sklearn.metrics.recall_score(target_label, predicted_label, pos_label=1)\n",
      "    if predicted_score == True:\n",
      "        auc_score  = roc_auc_score(target_label, predicted_label)\n",
      "        score['auc_score'] = auc_score\n",
      "        target_label = [x >= 0.5 for x in target_label]\n",
      "        score['accuracy'] = sklearn.metrics.accuracy_score(target_label, predicted_label)\n",
      "        score['precision'] = sklearn.metrics.precision_score(target_label, predicted_label, pos_label=1)\n",
      "        score['recall'] = sklearn.metrics.recall_score(target_label, predicted_label, pos_label=1)\n",
      "    if print_report == True:\n",
      "        for key, value in score.iteritems():\n",
      "            print key, '{percent:.1%}'.format(percent=value)\n",
      "    return score\n",
      "\n",
      "def LOO_out_performance_for_all(ddis):\n",
      "    for ddi in ddis:\n",
      "        process_one_ddi(ddi)\n",
      "def process_one_ddi(ddi):\n",
      "    \"\"\"A function to waste CPU cycles\"\"\"\n",
      "    logger.info('DDI: %s' % ddi)\n",
      "    one_ddi_family = {}\n",
      "    one_ddi_family[ddi] = LOO_out_performance_for_one_ddi(ddi)\n",
      "    one_ddi_family[ddi].get_LOO_perfermance('FisherM1', '')\n",
      "    return None\n",
      "def parallel_process(function, ddis, nproc = 2):\n",
      "    # maximum number of simultaneous processes desired\n",
      "    results = pprocess.Map(limit=nproc, reuse=1)\n",
      "    parallel_function = results.manage(pprocess.MakeReusable(function))\n",
      "    [parallel_function(ddi) for ddi in ddis]  # Start computing things\n",
      "    return results[:]\n",
      "\n",
      "class LOO_out_performance_for_one_ddi(object):\n",
      "        \"\"\" get the performance of ddi families\n",
      "        Attributes:\n",
      "            ddi: string ddi name\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string, folder\n",
      "            total_number_of_sequences: int\n",
      "            raw_data: dict raw_data[2]\n",
      "\n",
      "        \"\"\"\n",
      "        def __init__(self, ddi):\n",
      "            self.ddi_obj = DDI_family_base(ddi)\n",
      "            self.ddi = ddi\n",
      "        def analysis_score(self, target_label, predicted_label): #new\n",
      "            score = (sklearn.metrics.accuracy_score(target_label, predicted_label),\n",
      "                     sklearn.metrics.precision_score(target_label, predicted_label, pos_label=1),\n",
      "                     sklearn.metrics.recall_score(target_label, predicted_label, pos_label=1))\n",
      "            return score\n",
      "        def saveAsCsv(self, predicted_score, fname, *arguments): #new\n",
      "            newfile = False\n",
      "            logger.info('saving to file: %s' %fname)\n",
      "            if os.path.isfile(fname + '_report.csv'):\n",
      "                pass\n",
      "            else:\n",
      "                newfile = True\n",
      "            csvfile = open(fname + '_report.csv', 'a+')\n",
      "            writer = csv.writer(csvfile)\n",
      "            if newfile == True:\n",
      "                if predicted_score == False:\n",
      "                    writer.writerow(['DDI', 'no.', 'FisherMode', 'method', 'isTest', 'accuracy', 'precision', 'recall']) #, 'AUC'])\n",
      "                else:\n",
      "                    writer.writerow(['DDI', 'no.', 'FisherMode', 'method', 'isTest', 'AUC', 'accuracy', 'precision', 'recall'])\n",
      "            for arg in arguments:        \n",
      "                writer.writerows(arg)\n",
      "            csvfile.close()\n",
      "\n",
      "        def get_LOO_perfermance(self, fisher_mode, settings = None):\n",
      "            analysis_scr = []\n",
      "            predicted_score = False\n",
      "            reduce_ratio = 4\n",
      "            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):\n",
      "                print seq_no\n",
      "                print \"SVM\"\n",
      "                (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)\n",
      "                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)\n",
      "                scaled_train_X = standard_scaler.transform(train_X_reduced)\n",
      "                scaled_test_X = standard_scaler.transform(test_X)\n",
      "                Linear_SVC = LinearSVC(C=1, penalty=\"l2\")\n",
      "                Linear_SVC.fit(scaled_train_X, train_y_reduced)\n",
      "                predicted_test_y = Linear_SVC.predict(scaled_test_X)\n",
      "                isTest = True; #new\n",
      "                analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new\n",
      "                \n",
      "                \n",
      "                Linear_SVC = LinearSVC(C=1, penalty=\"l2\")\n",
      "                Linear_SVC.fit(scaled_train_X, train_y_reduced)\n",
      "                predicted_train_y = Linear_SVC.predict(scaled_train_X)\n",
      "                isTest = False; #new\n",
      "                analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))\n",
      "                \n",
      "                \n",
      "                print \"direct deep learning\"\n",
      "                # direct deep learning \n",
      "                min_max_scaler = Precessing_Scaler_0_9()\n",
      "                X_train_pre_validation_minmax = min_max_scaler.fit(train_X_reduced)\n",
      "                X_train_pre_validation_minmax = min_max_scaler.transform(train_X_reduced)\n",
      "                x_test_minmax = min_max_scaler.transform(test_X)\n",
      "                pretraining_X_minmax = min_max_scaler.transform(train_X_LOO)\n",
      "                x_train_minmax, x_validation_minmax, y_train_minmax, y_validation_minmax = train_test_split(X_train_pre_validation_minmax, \n",
      "                                                                                                  train_y_reduced\n",
      "                                                                    , test_size=0.2, random_state=42)\n",
      "                finetune_lr = 1\n",
      "                batch_size = 30\n",
      "                pretraining_epochs = cal_epochs(10000, x_train_minmax, batch_size = batch_size)\n",
      "                #pretrain_lr=0.001\n",
      "                pretrain_lr = 0.01\n",
      "                training_epochs = 1000\n",
      "                hidden_layers_sizes= [100, 100]\n",
      "                corruption_levels = [0,0]\n",
      "                sda = trainSda(x_train_minmax, y_train_minmax,\n",
      "                             x_validation_minmax, y_validation_minmax , \n",
      "                             x_test_minmax, test_y,\n",
      "                             hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                             training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                             pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                 )\n",
      "                print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                print 'corruption_levels:', corruption_levels\n",
      "                training_predicted = sda.predict(x_train_minmax)\n",
      "                y_train = y_train_minmax\n",
      "                isTest = False; #new\n",
      "                analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL', isTest) + tuple(performance_score(y_train, training_predicted).values()))\n",
      "                \n",
      "                test_predicted = sda.predict(x_test_minmax)\n",
      "                y_test = test_y\n",
      "                isTest = True; #new\n",
      "                analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL', isTest) + tuple(performance_score(y_test, test_predicted).values()))\n",
      "                \n",
      "                \n",
      "                # deep learning using unlabeled data for pretraining\n",
      "                print 'deep learning with unlabel data'\n",
      "                pretraining_epochs = cal_epochs(10000, pretraining_X_minmax, batch_size = batch_size)\n",
      "                sda_unlabel = trainSda(x_train_minmax, y_train_minmax,\n",
      "                             x_validation_minmax, y_validation_minmax , \n",
      "                             x_test_minmax, test_y, \n",
      "                             pretraining_X_minmax = pretraining_X_minmax,\n",
      "                             hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                             training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                             pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                 )\n",
      "                print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                print 'corruption_levels:', corruption_levels\n",
      "                training_predicted = sda_unlabel.predict(x_train_minmax)\n",
      "                y_train = y_train_minmax\n",
      "                isTest = False; #new\n",
      "                analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL_U', isTest) + tuple(performance_score(y_train, training_predicted, predicted_score).values()))\n",
      "                \n",
      "                test_predicted = sda_unlabel.predict(x_test_minmax)\n",
      "                y_test = test_y\n",
      "                \n",
      "                isTest = True; #new\n",
      "                analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL_U', isTest) + tuple(performance_score(y_test, test_predicted, predicted_score).values()))\n",
      "            report_name = filename + '_' + '_'.join(map(str, hidden_layers_sizes)) + \\\n",
      "                            '_' + str(pretrain_lr) + '_' + str(finetune_lr) + '_' + str(reduce_ratio)\n",
      "            self.saveAsCsv(predicted_score, report_name, analysis_scr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " 'DDI: %s' % ddi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "'DDI: 6PGD_int_NAD_binding_2'"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "process_one_ddi(ddis[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'process_one_ddi' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-463297ca20f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocess_one_ddi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'process_one_ddi' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "parallel_process(process_one_ddi, ddis, nproc = 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ald_Xan_dh_C_int_FAD_binding_5 20\n",
        "error return without exception set\n"
       ]
      },
      {
       "ename": "KeyError",
       "evalue": "20",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-b21b401df1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparallel_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_one_ddi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-5-b97ff39cb95a>\u001b[0m in \u001b[0;36mparallel_process\u001b[1;34m(function, ddis, nproc)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mparallel_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMakeReusable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mparallel_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mddi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mddis\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Start computing things\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLOO_out_performance_for_one_ddi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pprocess.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfinished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pprocess.pyc\u001b[0m in \u001b[0;36mstore\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_waiting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pprocess.pyc\u001b[0m in \u001b[0;36mstore_data\u001b[1;34m(self, channel)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;34m\"Accumulate the incoming data, associating results with channels.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pprocess.pyc\u001b[0m in \u001b[0;36mreceive\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pprocess.pyc\u001b[0m in \u001b[0;36m_receive\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 20"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOO_out_performance_for_all(ddis)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'LOO_out_performance_for_all' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-bc27e0896ebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLOO_out_performance_for_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'LOO_out_performance_for_all' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_ddi_family.get_LOO_perfermance('AAONLY', '1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'one_ddi_family' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-f1a291fe5526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mone_ddi_family\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_LOO_perfermance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AAONLY'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'one_ddi_family' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data =a\n",
      "a = data[:, 20:227]\n",
      "b = data[:, 247:454]\n",
      "X = np.hstack((a,b))\n",
      "print a.shape, b.shape, X.shape,  X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}