{
 "metadata": {
  "name": "",
  "signature": "sha256:76c8d3f09a79b10dccd2d9204b635315144de5b23fdce27e43b39258de048334"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import cPickle\n",
      "import numpy as np\n",
      "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,y_train = train_set\n",
      "\n",
      "X_valid,y_valid = valid_set\n",
      "X_total=np.vstack((X_train, X_valid))\n",
      "print(X_total.shape)\n",
      "y_total = np.concatenate([y_train, y_valid])\n",
      "print y_total.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(60000L, 784L)\n",
        "(60000L,)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "array_A =[]\n",
      "array_B =[]\n",
      "for i in range(1000000):\n",
      "    array_A.append(np.random.random_integers(0, 59999))\n",
      "    array_B.append(np.random.random_integers(0, 59999))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import gzip\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "from logistic_sgd import LogisticRegression, load_data\n",
      "from mlp import HiddenLayer\n",
      "from dA import dA\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "def shared_dataset(data_xy, borrow=True):\n",
      "    \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "    The reason we store our dataset in shared variables is to allow\n",
      "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "    is needed (the default behaviour if the data is not in a shared\n",
      "    variable) would lead to a large decrease in performance.\n",
      "    \"\"\"\n",
      "    data_x, data_y = data_xy\n",
      "    shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                           dtype=theano.config.floatX),\n",
      "                             borrow=borrow)\n",
      "    shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                           dtype=theano.config.floatX),\n",
      "                             borrow=borrow)\n",
      "    # When storing data on the GPU it has to be stored as floats\n",
      "    # therefore we will store the labels as ``floatX`` as well\n",
      "    # (``shared_y`` does exactly that). But during our computations\n",
      "    # we need them as ints (we use labels as index, and if they are\n",
      "    # floats it doesn't make sense) therefore instead of returning\n",
      "    # ``shared_y`` we will have to cast it to int. This little hack\n",
      "    # lets ous get around this issue\n",
      "    return shared_x, T.cast(shared_y, 'int32')\n",
      "def shared_dataset_X(data_x, borrow=True):\n",
      "    shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                           dtype=theano.config.floatX),\n",
      "                             borrow=borrow)\n",
      "    return shared_x\n",
      "class MultipleAEs(object):\n",
      "    \"\"\"Stacked denoising auto-encoder class that extract hi level features.\n",
      "    get the last hidden layer activation\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
      "                 hidden_layers_sizes=[500, 500],\n",
      "                 corruption_levels=[0.1, 0.1]):\n",
      "        \"\"\" This class is made to support a variable number of layers.\n",
      "\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: numpy random number generator used to draw initial\n",
      "                    weights\n",
      "\n",
      "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
      "        :param theano_rng: Theano random generator; if None is given one is\n",
      "                           generated based on a seed drawn from `rng`\n",
      "\n",
      "        :type n_ins: int\n",
      "        :param n_ins: dimension of the input to the sdA\n",
      "\n",
      "        :type n_layers_sizes: list of ints\n",
      "        :param n_layers_sizes: intermediate layers size, must contain\n",
      "        :type corruption_levels: list of float\n",
      "        :param corruption_levels: amount of corruption to use for each\n",
      "                                  layer\n",
      "        \"\"\"\n",
      "\n",
      "        self.sigmoid_layers = []\n",
      "        self.dA_layers = []\n",
      "        self.params = []\n",
      "        self.n_layers = len(hidden_layers_sizes)\n",
      "\n",
      "        assert self.n_layers > 0\n",
      "\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "        # allocate symbolic variables for the data\n",
      "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
      "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                                 # [int] labels\n",
      "\n",
      "        # The SdA is an MLP, for which all weights of intermediate layers\n",
      "        # are shared with a different denoising autoencoders\n",
      "        # We will first construct the SdA as a deep multilayer perceptron,\n",
      "        # and when constructing each sigmoidal layer we also construct a\n",
      "        # denoising autoencoder that shares weights with that layer\n",
      "        # During pretraining we will train these autoencoders (which will\n",
      "        # lead to chainging the weights of the MLP as well)\n",
      "        # During finetunining we will finish training the SdA by doing\n",
      "        # stochastich gradient descent on the MLP\n",
      "\n",
      "        for i in xrange(self.n_layers):\n",
      "            # construct the sigmoidal layer\n",
      "\n",
      "            # the size of the input is either the number of hidden units of\n",
      "            # the layer below or the input size if we are on the first layer\n",
      "            if i == 0:\n",
      "                input_size = n_ins\n",
      "            else:\n",
      "                input_size = hidden_layers_sizes[i - 1]\n",
      "\n",
      "            # the input to this layer is either the activation of the hidden\n",
      "            # layer below or the input of the SdA if you are on the first\n",
      "            # layer\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[-1].output\n",
      "\n",
      "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
      "                                        input=layer_input,\n",
      "                                        n_in=input_size,\n",
      "                                        n_out=hidden_layers_sizes[i],\n",
      "                                        activation=T.nnet.sigmoid)\n",
      "            # add the layer to our list of layers\n",
      "            self.sigmoid_layers.append(sigmoid_layer)\n",
      "            # its arguably a philosophical question...\n",
      "            # but we are going to only declare that the parameters of the\n",
      "            # sigmoid_layers are parameters of the StackedDAA\n",
      "            # the visible biases in the dA are parameters of those\n",
      "            # dA, but not the SdA\n",
      "            self.params.extend(sigmoid_layer.params)\n",
      "\n",
      "            # Construct a denoising autoencoder that shared weights with this\n",
      "            # layer\n",
      "            dA_layer = dA(numpy_rng=numpy_rng,\n",
      "                          theano_rng=theano_rng,\n",
      "                          input=layer_input,\n",
      "                          n_visible=input_size,\n",
      "                          n_hidden=hidden_layers_sizes[i],\n",
      "                          W=sigmoid_layer.W,\n",
      "                          bhid=sigmoid_layer.b)\n",
      "            self.dA_layers.append(dA_layer)\n",
      "        '''\n",
      "        we don't need this layer since is AE\n",
      "        # We now need to add a logistic layer on top of the MLP\n",
      "        self.logLayer = LogisticRegression(\n",
      "                         input=self.sigmoid_layers[-1].output,\n",
      "                         n_in=hidden_layers_sizes[-1], n_out=n_outs)\n",
      "        \n",
      "        self.params.extend(self.logLayer.params)\n",
      "        # construct a function that implements one step of finetunining\n",
      "        \n",
      "        # compute the cost for second phase of training,\n",
      "        # defined as the negative log likelihood\n",
      "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
      "        # compute the gradients with respect to the model parameters\n",
      "        # symbolic variable that points to the number of errors made on the\n",
      "        # minibatch given by self.x and self.y\n",
      "        self.errors = self.logLayer.errors(self.y)\n",
      "        '''\n",
      "    def transform(self,data_x): # get the last layaer activations to transform data.\n",
      "        last_layer_activations = self.sigmoid_layers[-1].output\n",
      "        theano_fn = theano.function(inputs=[],\n",
      "                                 outputs=last_layer_activations,\n",
      "\n",
      "                                 givens={self.x: data_x})\n",
      "        newFeatures=theano_fn()\n",
      "        return newFeatures\n",
      "    def pretraining_functions(self, train_set_x, batch_size):\n",
      "        ''' Generates a list of functions, each of them implementing one\n",
      "        step in trainnig the dA corresponding to the layer with same index.\n",
      "        The function will require as input the minibatch index, and to train\n",
      "        a dA you just need to iterate, calling the corresponding function on\n",
      "        all minibatch indexes.\n",
      "\n",
      "        :type train_set_x: theano.tensor.TensorType\n",
      "        :param train_set_x: Shared variable that contains all datapoints used\n",
      "                            for training the dA\n",
      "\n",
      "        :type batch_size: int\n",
      "        :param batch_size: size of a [mini]batch\n",
      "\n",
      "        :type learning_rate: float\n",
      "        :param learning_rate: learning rate used during training for any of\n",
      "                              the dA layers\n",
      "        '''\n",
      "\n",
      "        # index to a [mini]batch\n",
      "        index = T.lscalar('index')  # index to a minibatch\n",
      "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
      "        learning_rate = T.scalar('lr')  # learning rate to use\n",
      "        # number of batches\n",
      "        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "        # begining of a batch, given `index`\n",
      "        batch_begin = index * batch_size\n",
      "        # ending of a batch given `index`\n",
      "        batch_end = batch_begin + batch_size\n",
      "\n",
      "        pretrain_fns = []\n",
      "        for dA in self.dA_layers:\n",
      "            # get the cost and the updates list\n",
      "            cost, updates = dA.get_cost_updates(corruption_level,\n",
      "                                                learning_rate)\n",
      "            # compile the theano function\n",
      "            fn = theano.function(inputs=[index,\n",
      "                              theano.Param(corruption_level, default=0.2),\n",
      "                              theano.Param(learning_rate, default=0.1)],\n",
      "                                 outputs=cost,\n",
      "                                 updates=updates,\n",
      "                                 givens={self.x: train_set_x[batch_begin:\n",
      "                                                             batch_end]})\n",
      "            # append `fn` to the list of functions\n",
      "            pretrain_fns.append(fn)\n",
      "\n",
      "        return pretrain_fns\n",
      "\n",
      "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
      "        '''Generates a function `train` that implements one step of\n",
      "        finetuning, a function `validate` that computes the error on\n",
      "        a batch from the validation set, and a function `test` that\n",
      "        computes the error on a batch from the testing set\n",
      "\n",
      "        :type datasets: list of pairs of theano.tensor.TensorType\n",
      "        :param datasets: It is a list that contain all the datasets;\n",
      "                         the has to contain three pairs, `train`,\n",
      "                         `valid`, `test` in this order, where each pair\n",
      "                         is formed of two Theano variables, one for the\n",
      "                         datapoints, the other for the labels\n",
      "\n",
      "        :type batch_size: int\n",
      "        :param batch_size: size of a minibatch\n",
      "\n",
      "        :type learning_rate: float\n",
      "        :param learning_rate: learning rate used during finetune stage\n",
      "        '''\n",
      "\n",
      "        (train_set_x, train_set_y) = datasets[0]\n",
      "        (valid_set_x, valid_set_y) = datasets[1]\n",
      "        (test_set_x, test_set_y) = datasets[2]\n",
      "\n",
      "        # compute number of minibatches for training, validation and testing\n",
      "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
      "        n_valid_batches /= batch_size\n",
      "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
      "        n_test_batches /= batch_size\n",
      "\n",
      "        index = T.lscalar('index')  # index to a [mini]batch\n",
      "\n",
      "        # compute the gradients with respect to the model parameters\n",
      "        gparams = T.grad(self.finetune_cost, self.params)\n",
      "\n",
      "        # compute list of fine-tuning updates\n",
      "        updates = []\n",
      "        for param, gparam in zip(self.params, gparams):\n",
      "            updates.append((param, param - gparam * learning_rate))\n",
      "\n",
      "        train_fn = theano.function(inputs=[index],\n",
      "              outputs=self.finetune_cost,\n",
      "              updates=updates,\n",
      "              givens={\n",
      "                self.x: train_set_x[index * batch_size:\n",
      "                                    (index + 1) * batch_size],\n",
      "                self.y: train_set_y[index * batch_size:\n",
      "                                    (index + 1) * batch_size]},\n",
      "              name='train')\n",
      "\n",
      "        test_score_i = theano.function([index], self.errors,\n",
      "                 givens={\n",
      "                   self.x: test_set_x[index * batch_size:\n",
      "                                      (index + 1) * batch_size],\n",
      "                   self.y: test_set_y[index * batch_size:\n",
      "                                      (index + 1) * batch_size]},\n",
      "                      name='test')\n",
      "\n",
      "        valid_score_i = theano.function([index], self.errors,\n",
      "              givens={\n",
      "                 self.x: valid_set_x[index * batch_size:\n",
      "                                     (index + 1) * batch_size],\n",
      "                 self.y: valid_set_y[index * batch_size:\n",
      "                                     (index + 1) * batch_size]},\n",
      "                      name='valid')\n",
      "\n",
      "        # Create a function that scans the entire validation set\n",
      "        def valid_score():\n",
      "            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n",
      "\n",
      "        # Create a function that scans the entire test set\n",
      "        def test_score():\n",
      "            return [test_score_i(i) for i in xrange(n_test_batches)]\n",
      "\n",
      "\n",
      "        return train_fn, valid_score, test_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_a_MultipleAEs(X, pretraining_epochs=10, pretrain_lr=0.001, batch_size=30,\n",
      "                        hidden_layers_sizes=[100, 100], corruption_levels=[0, 0]):\n",
      "    \n",
      "    # get a shared copy of X\n",
      "    train_set_x = shared_dataset_X(X)\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
      "    n_train_batches /= batch_size\n",
      "\n",
      "    # numpy random generator\n",
      "    numpy_rng = numpy.random.RandomState(89677)\n",
      "    print '... building the model'\n",
      "    # construct the stacked denoising autoencoder class\n",
      "    sda = MultipleAEs(numpy_rng=numpy_rng, n_ins=train_set_x.get_value(borrow=True).shape[1],\n",
      "              hidden_layers_sizes=hidden_layers_sizes, corruption_levels = corruption_levels)\n",
      "\n",
      "    #########################\n",
      "    # PRETRAINING THE MODEL #\n",
      "    #########################\n",
      "    print '... getting the pretraining functions'\n",
      "    pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,\n",
      "                                                batch_size=batch_size)\n",
      "\n",
      "    print '... pre-training the model'\n",
      "    start_time = time.clock()\n",
      "    ## Pre-train layer-wise\n",
      "    corruption_levels = [.1, .2, .3]\n",
      "    for i in xrange(sda.n_layers):\n",
      "        # go through pretraining epochs\n",
      "        for epoch in xrange(pretraining_epochs):\n",
      "            # go through the training set\n",
      "            c = []\n",
      "            for batch_index in xrange(n_train_batches):\n",
      "                c.append(pretraining_fns[i](index=batch_index,\n",
      "                         corruption=corruption_levels[i],\n",
      "                         lr=pretrain_lr))\n",
      "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
      "            print numpy.mean(c)\n",
      "\n",
      "    end_time = time.clock()\n",
      "    return sda"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DL_libs import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is a use case.\n",
      "pretraining_epochs=1\n",
      "pretrain_lr=0.001\n",
      "batch_size=30\n",
      "hidden_layers_sizes =[90, 90]\n",
      "corruption_levels=[0, 0]\n",
      "x =numpy.random.random((1000, 50)) \n",
      "print \"original shape\", x.shape\n",
      "a_MAE = train_a_MultipleAEs(x, pretraining_epochs=pretraining_epochs, pretrain_lr=pretrain_lr, batch_size=batch_size, \n",
      "                        hidden_layers_sizes =hidden_layers_sizes, corruption_levels=corruption_levels)\n",
      "print \"transformed shape\", a_MAE.transform(x).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "original shape (1000L, 50L)\n",
        "... building the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... getting the pretraining functions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... pre-training the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pre-training layer 0, epoch 0, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 56.7114979571\n",
        "Pre-training layer 1, epoch 0, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 98.1964756011\n",
        "transformed shape "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1000L, 90L)\n"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}