{
 "metadata": {
  "name": "",
  "signature": "sha256:7c29b9a6cf43a38d6527c3850a57326fe8c0dd7a1bbf9e8370ec17ababdb7753"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "sys.path.append('../../../libs/')\n",
      "import os.path\n",
      "import IO_class\n",
      "from IO_class import FileOperator\n",
      "from sklearn import cross_validation\n",
      "import sklearn\n",
      "import numpy as np\n",
      "import csv\n",
      "from dateutil import parser\n",
      "from datetime import timedelta\n",
      "from sklearn import svm\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pdb\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn import preprocessing\n",
      "import sklearn\n",
      "import scipy.stats as ss\n",
      "from sklearn.svm import LinearSVC\n",
      "import random\n",
      "from DL_libs import *\n",
      "from itertools import izip #new\n",
      "import math\n",
      "from sklearn.svm import SVC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filename = 'SUCCESS_log_CrossValidation_load_DL_remoteFisherM1_DL_RE_US_DL_RE_US_1_1_19MAY2014.txt'\n",
      "#filename = 'listOfDDIsHaveOver2InterfacesHave40-75_Examples_2010_real_selected.txt' #for testing\n",
      "filename = 'ddi_examples_40_60_over2top_diff_name_2014.txt'\n",
      "file_obj = FileOperator(filename)\n",
      "ddis = file_obj.readStripLines()\n",
      "import logging\n",
      "import time\n",
      "current_date = time.strftime(\"%m_%d_%Y\")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "logname = 'log_DL_contact_matrix_load' + current_date + '.log'\n",
      "handler = logging.FileHandler(logname)\n",
      "handler.setLevel(logging.DEBUG)\n",
      "\n",
      "# create a logging format\n",
      "\n",
      "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
      "handler.setFormatter(formatter)\n",
      "\n",
      "# add the handlers to the logger\n",
      "\n",
      "logger.addHandler(handler)\n",
      "\n",
      "logger.info('Input DDI file: ' + filename)\n",
      "#logger.debug('This message should go to the log file')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:__main__:Input DDI file: listOfDDIsHaveOver2InterfacesHave40-75_Examples_2010_real_selected.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of lines in listOfDDIsHaveOver2InterfacesHave40-75_Examples_2010_real_selected.txt:1\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ddis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['6PGD_int_NAD_binding_2']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DDI_family_base(object):\n",
      "    #def __init__(self, ddi, Vectors_Fishers_aaIndex_raw_folder = '/home/du/Documents/Vectors_Fishers_aaIndex_raw_2014/'):\n",
      "    #def __init__(self, ddi, Vectors_Fishers_aaIndex_raw_folder = '/home/sun/Downloads/contactmatrix/contactmatrixanddeeplearningcode/data_test/'):\n",
      "    def __init__(self, ddi, Vectors_Fishers_aaIndex_raw_folder = '/big/du/Protein_Protein_Interaction_Project/Contact_Matrix_Project/Vectors_Fishers_aaIndex_raw_2014_paper/'):\n",
      "        \"\"\" get total number of sequences in a ddi familgy\n",
      "        Attributes:\n",
      "            ddi: string ddi name\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string, folder\n",
      "            total_number_of_sequences: int\n",
      "            raw_data: dict raw_data[2]\n",
      "        LOO_data['FisherM1'][1]\n",
      "\n",
      "        \"\"\"\n",
      "        self.ddi = ddi\n",
      "        self.Vectors_Fishers_aaIndex_raw_folder = Vectors_Fishers_aaIndex_raw_folder\n",
      "        self.ddi_folder = self.Vectors_Fishers_aaIndex_raw_folder + ddi + '/'\n",
      "        self.total_number_of_sequences = self.get_total_number_of_sequences()\n",
      "        self.raw_data = {}\n",
      "        self.positve_negative_number = {}\n",
      "        self.equal_size_data = {}\n",
      "        for seq_no in range(1, self.total_number_of_sequences+1):\n",
      "            self.raw_data[seq_no] = self.get_raw_data_for_selected_seq(seq_no)\n",
      "            try:\n",
      "                #positive_file = self.ddi_folder + 'numPos_'+ str(seq_no) + '.txt'\n",
      "                #file_obj = FileOperator(positive_file)\n",
      "                #lines = file_obj.readStripLines()\n",
      "                #import pdb; pdb.set_trace()\n",
      "                count_pos = int(np.sum(self.raw_data[seq_no][:, -1]))\n",
      "                count_neg = self.raw_data[seq_no].shape[0] - count_pos\n",
      "                #self.positve_negative_number[seq_no] = {'numPos': int(float(lines[0]))}\n",
      "                #assert int(float(lines[0])) == count_pos\n",
      "                self.positve_negative_number[seq_no] = {'numPos': count_pos}\n",
      "                #negative_file = self.ddi_folder + 'numNeg_'+ str(seq_no) + '.txt'\n",
      "                #file_obj = FileOperator(negative_file)\n",
      "                #lines = file_obj.readStripLines()\n",
      "                #self.positve_negative_number[seq_no]['numNeg'] =  int(float(lines[0]))\n",
      "                self.positve_negative_number[seq_no]['numNeg'] =  count_neg\n",
      "            except Exception,e:\n",
      "                print ddi, seq_no\n",
      "                print str(e)\n",
      "                logger.info(ddi + str(seq_no))\n",
      "                logger.info(str(e))                \n",
      "            # get data for equal positive and negative\n",
      "            n_pos = self.positve_negative_number[seq_no]['numPos']\n",
      "            n_neg = self.positve_negative_number[seq_no]['numNeg']\n",
      "            index_neg = range(n_pos, n_pos + n_neg)\n",
      "            random.shuffle(index_neg)\n",
      "            index_neg = index_neg[: n_pos]\n",
      "            positive_examples = self.raw_data[seq_no][ : n_pos, :]\n",
      "            negative_examples = self.raw_data[seq_no][index_neg, :]\n",
      "            self.equal_size_data[seq_no] = np.vstack((positive_examples, negative_examples))\n",
      "    def get_LOO_training_and_reduced_traing(self, seq_no, fisher_mode = 'FisherM1ONLY' , reduce_ratio = 4):\n",
      "        \"\"\" get the leave one out traing data, reduced traing\n",
      "        Parameters:\n",
      "            seq_no: \n",
      "            fisher_mode: default 'FisherM1ONLY'\n",
      "        Returns:\n",
      "            (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced),  (test_X, test_y)\n",
      "        \"\"\"\n",
      "        train_X_LOO = np.array([])\n",
      "        train_y_LOO = np.array([])\n",
      "        train_X_reduced = np.array([])\n",
      "        train_y_reduced = np.array([])\n",
      "\n",
      "        total_number_of_sequences = self.total_number_of_sequences\n",
      "        equal_size_data_selected_sequence = self.equal_size_data[seq_no]\n",
      "        \n",
      "        #get test data for selected sequence\n",
      "        test_X, test_y = self.select_X_y(equal_size_data_selected_sequence, fisher_mode = fisher_mode)\n",
      "        total_sequences = range(1, total_number_of_sequences+1)\n",
      "        loo_sequences = [i for i in total_sequences if i != seq_no]\n",
      "        number_of_reduced = len(loo_sequences)/reduce_ratio if len(loo_sequences)/reduce_ratio !=0 else 1\n",
      "        random.shuffle(loo_sequences)\n",
      "        reduced_sequences = loo_sequences[:number_of_reduced]\n",
      "\n",
      "        #for loo data\n",
      "        for current_no in loo_sequences:\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if train_X_LOO.ndim ==1:\n",
      "                train_X_LOO = current_X\n",
      "            else:\n",
      "                train_X_LOO = np.vstack((train_X_LOO, current_X))\n",
      "            train_y_LOO = np.concatenate((train_y_LOO, current_y))\n",
      "\n",
      "        #for reduced data\n",
      "        for current_no in reduced_sequences:\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if train_X_reduced.ndim ==1:\n",
      "                train_X_reduced = current_X\n",
      "            else:\n",
      "                train_X_reduced = np.vstack((train_X_reduced, current_X))\n",
      "            train_y_reduced = np.concatenate((train_y_reduced, current_y))                \n",
      "\n",
      "        return (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y)\n",
      "        \n",
      "    #def get_ten_fold_crossvalid_one_subset(self, start_subset, end_subset, fisher_mode = 'FisherM1ONLY' , reduce_ratio = 4):\n",
      "    def get_ten_fold_crossvalid_one_subset(self, train_index, test_index, fisher_mode = 'FisherM1ONLY' , reduce_ratio = 4):\n",
      "        \"\"\" get traing data, reduced traing data for 10-fold crossvalidation\n",
      "        Parameters:\n",
      "            start_subset: index of start of the testing data\n",
      "            end_subset: index of end of the testing data\n",
      "            fisher_mode: default 'FisherM1ONLY'\n",
      "        Returns:\n",
      "            (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced),  (test_X, test_y)\n",
      "        \"\"\"\n",
      "        train_X_10fold = np.array([])\n",
      "        train_y_10fold = np.array([])\n",
      "        train_X_reduced = np.array([])\n",
      "        train_y_reduced = np.array([])\n",
      "        test_X = np.array([])\n",
      "        test_y = np.array([])\n",
      "\n",
      "        total_number_of_sequences = self.total_number_of_sequences\n",
      "        \n",
      "        #get test data for selected sequence\n",
      "        #for current_no in range(start_subset, end_subset):\n",
      "        for num in test_index:\n",
      "            current_no = num + 1\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if test_X.ndim ==1:\n",
      "                test_X = current_X\n",
      "            else:\n",
      "                test_X = np.vstack((test_X, current_X))\n",
      "            test_y = np.concatenate((test_y, current_y))\n",
      "        \n",
      "        #total_sequences = range(1, total_number_of_sequences+1)\n",
      "        #ten_fold_sequences = [i for i in total_sequences if not(i in range(start_subset, end_subset))]\n",
      "        #number_of_reduced = len(ten_fold_sequences)/reduce_ratio if len(ten_fold_sequences)/reduce_ratio !=0 else 1\n",
      "        #random.shuffle(ten_fold_sequences)\n",
      "        #reduced_sequences = ten_fold_sequences[:number_of_reduced]\n",
      "        \n",
      "        number_of_reduced = len(train_index)/reduce_ratio if len(train_index)/reduce_ratio !=0 else 1\n",
      "        random.shuffle(train_index)\n",
      "        reduced_sequences = train_index[:number_of_reduced]\n",
      "\n",
      "        #for 10-fold cross-validation data\n",
      "        #for current_no in ten_fold_sequences:\n",
      "        for num in train_index:\n",
      "            current_no = num + 1\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if train_X_10fold.ndim ==1:\n",
      "                train_X_10fold = current_X\n",
      "            else:\n",
      "                train_X_10fold = np.vstack((train_X_10fold, current_X))\n",
      "            train_y_10fold = np.concatenate((train_y_10fold, current_y))\n",
      "\n",
      "        #for reduced data\n",
      "        for num in reduced_sequences:\n",
      "            current_no = num + 1\n",
      "            raw_current_data = self.equal_size_data[current_no]\n",
      "            current_X, current_y = self.select_X_y(raw_current_data, fisher_mode = fisher_mode)\n",
      "            if train_X_reduced.ndim ==1:\n",
      "                train_X_reduced = current_X\n",
      "            else:\n",
      "                train_X_reduced = np.vstack((train_X_reduced, current_X))\n",
      "            train_y_reduced = np.concatenate((train_y_reduced, current_y))                \n",
      "\n",
      "        return (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y)\n",
      "        \n",
      "    def get_total_number_of_sequences(self):\n",
      "        \"\"\" get total number of sequences in a ddi familgy\n",
      "        Parameters:\n",
      "            ddi: string\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string\n",
      "        Returns:\n",
      "            n: int\n",
      "        \"\"\"\n",
      "        folder_path = self.Vectors_Fishers_aaIndex_raw_folder + self.ddi + '/' \n",
      "        filename = folder_path +'allPairs.txt'\n",
      "        all_pairs = np.loadtxt(filename)\n",
      "        return len(all_pairs)\n",
      "\n",
      "    def get_raw_data_for_selected_seq(self, seq_no):\n",
      "        \"\"\" get raw data for selected seq no in a family\n",
      "        Parameters:\n",
      "            ddi: \n",
      "            seq_no: \n",
      "        Returns:\n",
      "            data: raw data in the sequence file\n",
      "        \"\"\"\n",
      "        folder_path = self.Vectors_Fishers_aaIndex_raw_folder + self.ddi + '/' \n",
      "        filename = folder_path + 'F0_20_F1_20_Sliding_17_11_F0_20_F1_20_Sliding_17_11_ouput_'+ str(seq_no) + '.txt'\n",
      "        data = np.loadtxt(filename)\n",
      "        return data\n",
      "    def select_X_y(self, data, fisher_mode = ''):\n",
      "        \"\"\" select subset from the raw input data set\n",
      "        Parameters:\n",
      "            data: data from matlab txt file\n",
      "            fisher_mode: subset base on this Fisher of AAONLY...\n",
      "        Returns:\n",
      "            selected X,  y\n",
      "        \"\"\"\n",
      "        y = data[:,-1] # get lable\n",
      "        if fisher_mode == 'FisherM1': # fisher m1 plus AA index\n",
      "            a = data[:, 20:227]\n",
      "            b = data[:, 247:454]\n",
      "            X = np.hstack((a,b))\n",
      "        elif fisher_mode == 'FisherM1ONLY': \n",
      "            a = data[:, 20:40]\n",
      "            b = data[:, 247:267]\n",
      "            X = np.hstack((a,b))\n",
      "        elif fisher_mode == 'AAONLY':\n",
      "            a = data[:, 40:227]\n",
      "            b = data[:, 267:454]\n",
      "            X = np.hstack((a,b))\n",
      "        else:\n",
      "            raise('there is an error in mode')\n",
      "        return X, y\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import sklearn.preprocessing\n",
      "class Precessing_Scaler_0_9(sklearn.preprocessing.StandardScaler):\n",
      "    def __init__(self):\n",
      "        super(Precessing_Scaler_0_9, self).__init__(self, with_std=0.333)\n",
      "    def transform(self, X): # transform data to 0.1 to 0.9\n",
      "        new_X = super(Precessing_Scaler_0_9, self).transform(X)\n",
      "        print \n",
      "        new_X[new_X > 1] = 1\n",
      "        new_X[new_X < -1] = -1\n",
      "        new_X = (new_X + 1) * 0.4 + 0.1\n",
      "        return new_X\n",
      "    def fit_transform(self):\n",
      "        print 'Did not implement'\n",
      "def performance_score(target_label, predicted_label, predicted_score = False, print_report = True): \n",
      "    \"\"\" get performance matrix for prediction\n",
      "        Attributes:\n",
      "            target_label: int 0, 1\n",
      "            predicted_label: 0, 1 or ranking\n",
      "            predicted_score: bool if False, predicted_label is from 0, 1. If Ture, predicted_label is ranked, need to get AUC score.\n",
      "            print_report: if True, print the perfromannce on screen\n",
      "    \"\"\"\n",
      "    import sklearn\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "    score = {}\n",
      "    if predicted_score == False:\n",
      "        score['accuracy'] = sklearn.metrics.accuracy_score(target_label, predicted_label)\n",
      "        score['precision'] = sklearn.metrics.precision_score(target_label, predicted_label, pos_label=1)\n",
      "        score['recall'] = sklearn.metrics.recall_score(target_label, predicted_label, pos_label=1)\n",
      "    if predicted_score == True:\n",
      "        auc_score  = roc_auc_score(target_label, predicted_label)\n",
      "        score['auc_score'] = auc_score\n",
      "        target_label = [x >= 0.5 for x in target_label]\n",
      "        score['accuracy'] = sklearn.metrics.accuracy_score(target_label, predicted_label)\n",
      "        score['precision'] = sklearn.metrics.precision_score(target_label, predicted_label, pos_label=1)\n",
      "        score['recall'] = sklearn.metrics.recall_score(target_label, predicted_label, pos_label=1)\n",
      "    if print_report == True:\n",
      "        for key, value in score.iteritems():\n",
      "            print key, '{percent:.1%}'.format(percent=value)\n",
      "    return score\n",
      "\n",
      "def saveAsCsv(predicted_score, fname, score_dict, *arguments): #new\n",
      "    newfile = False\n",
      "    if os.path.isfile('report_' + fname + '.csv'):\n",
      "        pass\n",
      "    else:\n",
      "        newfile = True\n",
      "    csvfile = open('report_' + fname + '.csv', 'a+')\n",
      "    writer = csv.writer(csvfile)\n",
      "    if newfile == True:\n",
      "        if predicted_score == False:\n",
      "            writer.writerow(['DDI', 'no.', 'FisherMode', 'method', 'isTest']+ score_dict.keys()) #, 'AUC'])\n",
      "        else:\n",
      "            writer.writerow(['DDI', 'no.', 'FisherMode', 'method', 'isTest'] + score_dict.keys())\n",
      "    for arg in arguments:        \n",
      "        writer.writerows(arg)\n",
      "    csvfile.close()\n",
      "\n",
      "def LOO_out_performance_for_all(ddis):\n",
      "    for ddi in ddis:\n",
      "        try:\n",
      "            one_ddi_family = LOO_out_performance_for_one_ddi(ddi)\n",
      "            one_ddi_family.get_LOO_perfermance('FisherM1', '')\n",
      "        except Exception,e:\n",
      "            print str(e)\n",
      "            logger.info(\"There is a error in this ddi: %s\" % ddi)\n",
      "            logger.info(str(e))\n",
      "\n",
      "        \n",
      "class LOO_out_performance_for_one_ddi(object):\n",
      "        \"\"\" get the performance of ddi families\n",
      "        Attributes:\n",
      "            ddi: string ddi name\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string, folder\n",
      "            total_number_of_sequences: int\n",
      "            raw_data: dict raw_data[2]\n",
      "\n",
      "        \"\"\"\n",
      "        def __init__(self, ddi):\n",
      "            self.ddi_obj = DDI_family_base(ddi)\n",
      "            self.ddi = ddi\n",
      "\n",
      "        def get_LOO_perfermance(self, fisher_mode, settings = None):\n",
      "            analysis_scr = []\n",
      "            predicted_score = False\n",
      "            reduce_ratio = 1\n",
      "            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):\n",
      "                print seq_no\n",
      "                logger.info('sequence number: ' + str(seq_no))\n",
      "                if 1:\n",
      "                    print \"SVM\"\n",
      "                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)\n",
      "                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)\n",
      "                    scaled_train_X = standard_scaler.transform(train_X_reduced)\n",
      "                    scaled_test_X = standard_scaler.transform(test_X)\n",
      "                    Linear_SVC = LinearSVC(C=1, penalty=\"l2\")\n",
      "                    Linear_SVC.fit(scaled_train_X, train_y_reduced)\n",
      "                    predicted_test_y = Linear_SVC.predict(scaled_test_X)\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new\n",
      "\n",
      "                    predicted_train_y = Linear_SVC.predict(scaled_train_X)\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))\n",
      "                # Deep learning part\n",
      "                min_max_scaler = Precessing_Scaler_0_9()\n",
      "                X_train_pre_validation_minmax = min_max_scaler.fit(train_X_reduced)\n",
      "                X_train_pre_validation_minmax = min_max_scaler.transform(train_X_reduced)\n",
      "                x_test_minmax = min_max_scaler.transform(test_X)\n",
      "                pretraining_X_minmax = min_max_scaler.transform(train_X_LOO)\n",
      "                x_train_minmax, x_validation_minmax, y_train_minmax, y_validation_minmax = train_test_split(X_train_pre_validation_minmax, \n",
      "                                                                                                  train_y_reduced\n",
      "                                                                    , test_size=0.4, random_state=42)\n",
      "                finetune_lr = 1\n",
      "                batch_size = 100\n",
      "                pretraining_epochs = cal_epochs(1500, x_train_minmax, batch_size = batch_size)\n",
      "                #pretrain_lr=0.001\n",
      "                pretrain_lr = 0.001\n",
      "                training_epochs = 1501\n",
      "                hidden_layers_sizes= [100, 100]\n",
      "                corruption_levels = [0,0]\n",
      "                if 1:\n",
      "                    print \"direct deep learning\"\n",
      "                    # direct deep learning \n",
      "                    sda = trainSda(x_train_minmax, y_train_minmax,\n",
      "                                 x_validation_minmax, y_validation_minmax , \n",
      "                                 x_test_minmax, test_y,\n",
      "                                 hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                                 training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                                 pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                     )\n",
      "                    print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                    print 'corruption_levels:', corruption_levels\n",
      "                    training_predicted = sda.predict(x_train_minmax)\n",
      "                    y_train = y_train_minmax\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL', isTest) + tuple(performance_score(y_train, training_predicted).values()))\n",
      "\n",
      "                    test_predicted = sda.predict(x_test_minmax)\n",
      "                    y_test = test_y\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL', isTest) + tuple(performance_score(y_test, test_predicted).values()))\n",
      "\n",
      "                if 0:\n",
      "                    # deep learning using unlabeled data for pretraining\n",
      "                    print 'deep learning with unlabel data'\n",
      "                    pretraining_epochs_for_reduced = cal_epochs(1500, pretraining_X_minmax, batch_size = batch_size)\n",
      "                    sda_unlabel = trainSda(x_train_minmax, y_train_minmax,\n",
      "                                 x_validation_minmax, y_validation_minmax , \n",
      "                                 x_test_minmax, test_y, \n",
      "                                 pretraining_X_minmax = pretraining_X_minmax,\n",
      "                                 hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                                 training_epochs = training_epochs, pretraining_epochs = pretraining_epochs_for_reduced, \n",
      "                                 pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                     )\n",
      "                    print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                    print 'corruption_levels:', corruption_levels\n",
      "                    training_predicted = sda_unlabel.predict(x_train_minmax)\n",
      "                    y_train = y_train_minmax\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL_U', isTest) + tuple(performance_score(y_train, training_predicted, predicted_score).values()))\n",
      "\n",
      "                    test_predicted = sda_unlabel.predict(x_test_minmax)\n",
      "                    y_test = test_y\n",
      "\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL_U', isTest) + tuple(performance_score(y_test, test_predicted, predicted_score).values()))\n",
      "                if 0:\n",
      "                    # deep learning using split network\n",
      "                    print 'deep learning using split network'\n",
      "                    # get the new representation for A set. first 784-D\n",
      "                    pretraining_epochs=1500\n",
      "                    hidden_layers_sizes =[50, 50]\n",
      "                    corruption_levels=[0, 0]\n",
      "                    \n",
      "                    x = x_train_minmax[:, :x_train_minmax.shape[1]/2]\n",
      "                    print \"original shape for A\", x.shape\n",
      "                    a_MAE_A = train_a_MultipleAEs(x, pretraining_epochs=pretraining_epochs, pretrain_lr=pretrain_lr, batch_size=batch_size, \n",
      "                                            hidden_layers_sizes =hidden_layers_sizes, corruption_levels=corruption_levels)\n",
      "                    new_x_train_minmax_A =  a_MAE_A.transform(x_train_minmax[:, :x_train_minmax.shape[1]/2])\n",
      "                    x = x_train_minmax[:, x_train_minmax.shape[1]/2:]\n",
      "                    \n",
      "                    print \"original shape for B\", x.shape\n",
      "                    a_MAE_B = train_a_MultipleAEs(x, pretraining_epochs=pretraining_epochs, pretrain_lr=pretrain_lr, batch_size=batch_size, \n",
      "                                            hidden_layers_sizes =hidden_layers_sizes, corruption_levels=corruption_levels)\n",
      "                    new_x_train_minmax_B =  a_MAE_B.transform(x_train_minmax[:, x_train_minmax.shape[1]/2:])\n",
      "                    \n",
      "                    new_x_test_minmax_A = a_MAE_A.transform(x_test_minmax[:, :x_test_minmax.shape[1]/2])\n",
      "                    new_x_test_minmax_B = a_MAE_B.transform(x_test_minmax[:, x_test_minmax.shape[1]/2:])\n",
      "                    new_x_validation_minmax_A = a_MAE_A.transform(x_validation_minmax[:, :x_validation_minmax.shape[1]/2])\n",
      "                    new_x_validation_minmax_B = a_MAE_B.transform(x_validation_minmax[:, x_validation_minmax.shape[1]/2:])\n",
      "                    new_x_train_minmax_whole = np.hstack((new_x_train_minmax_A, new_x_train_minmax_B))\n",
      "                    new_x_test_minmax_whole = np.hstack((new_x_test_minmax_A, new_x_test_minmax_B))\n",
      "                    new_x_validationt_minmax_whole = np.hstack((new_x_validation_minmax_A, new_x_validation_minmax_B))\n",
      "\n",
      "                    finetune_lr = 1\n",
      "                    batch_size = 100\n",
      "                    pretraining_epochs = cal_epochs(1500, x_train_minmax, batch_size = batch_size)\n",
      "                    #pretrain_lr=0.001\n",
      "                    pretrain_lr = 0.001\n",
      "                    training_epochs = 1500\n",
      "                    hidden_layers_sizes= [100, 100]\n",
      "                    corruption_levels = [0,0]\n",
      "                    \n",
      "                    sda_transformed = trainSda(new_x_train_minmax_whole, y_train_minmax,\n",
      "                         new_x_validationt_minmax_whole, y_validation_minmax , \n",
      "                         new_x_test_minmax_whole, y_test,\n",
      "                         hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                         training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                         pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                         )\n",
      "                    \n",
      "                    print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                    print 'corruption_levels:', corruption_levels\n",
      "                    training_predicted = sda_transformed.predict(new_x_train_minmax_whole)\n",
      "                    y_train = y_train_minmax\n",
      "                    \n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL_S', isTest) + tuple(performance_score(y_train, training_predicted, predicted_score).values()))\n",
      "\n",
      "                    test_predicted = sda_transformed.predict(new_x_test_minmax_whole)\n",
      "                    y_test = test_y\n",
      "\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'DL_S', isTest) + tuple(performance_score(y_test, test_predicted, predicted_score).values()))\n",
      "            \n",
      "            \n",
      "            \n",
      "            report_name = filename + '_' + '_'.join(map(str, hidden_layers_sizes)) + \\\n",
      "                            '_' + str(pretrain_lr) + '_' + str(finetune_lr) + '_' + str(reduce_ratio)+ \\\n",
      "                            '_' +str(training_epochs) + '_' + current_date\n",
      "            saveAsCsv(predicted_score, report_name, performance_score(y_test, test_predicted, predicted_score), analysis_scr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#for 10-fold cross validation\n",
      "\n",
      "def ten_fold_crossvalid_performance_for_all(ddis):\n",
      "    for ddi in ddis:\n",
      "        try:\n",
      "            process_one_ddi_tenfold(ddi)\n",
      "        except Exception,e:\n",
      "            print str(e)\n",
      "            logger.debug(\"There is a error in this ddi: %s\" % ddi)\n",
      "            logger.info(str(e))\n",
      "def process_one_ddi_tenfold(ddi):\n",
      "    \"\"\"A function to waste CPU cycles\"\"\"\n",
      "    logger.info('DDI: %s' % ddi)\n",
      "    one_ddi_family = {}\n",
      "    one_ddi_family[ddi] = Ten_fold_crossvalid_performance_for_one_ddi(ddi)\n",
      "    one_ddi_family[ddi].get_ten_fold_crossvalid_perfermance('FisherM1', '')\n",
      "    return None\n",
      "class Ten_fold_crossvalid_performance_for_one_ddi(object):\n",
      "        \"\"\" get the performance of ddi families\n",
      "        Attributes:\n",
      "            ddi: string ddi name\n",
      "            Vectors_Fishers_aaIndex_raw_folder: string, folder\n",
      "            total_number_of_sequences: int\n",
      "            raw_data: dict raw_data[2]\n",
      "\n",
      "        \"\"\"\n",
      "        def __init__(self, ddi):\n",
      "            self.ddi_obj = DDI_family_base(ddi)\n",
      "            self.ddi = ddi\n",
      "        def get_ten_fold_crossvalid_perfermance(self, fisher_mode, settings = None):\n",
      "            analysis_scr = []\n",
      "            predicted_score = False\n",
      "            reduce_ratio = 1\n",
      "            #for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):\n",
      "            #subset_size = math.floor(self.ddi_obj.total_number_of_sequences / 10.0)\n",
      "            kf = KFold(self.ddi_obj.total_number_of_sequences, n_folds = 10)\n",
      "            #for subset_no in range(1, 11):\n",
      "            for ((train_index, test_index),subset_no) in izip(kf,range(1,11)):\n",
      "            #for train_index, test_index in kf;\n",
      "                print(\"Subset:\", subset_no)\n",
      "                print(\"Train index: \", train_index)\n",
      "                print(\"Test index: \", test_index)\n",
      "                #logger.info('subset number: ' + str(subset_no))\n",
      "                if 1:\n",
      "                    print \"SVM\"\n",
      "                    (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, reduce_ratio = reduce_ratio)\n",
      "                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)\n",
      "                    scaled_train_X = standard_scaler.transform(train_X_reduced)\n",
      "                    scaled_test_X = standard_scaler.transform(test_X)\n",
      "                    Linear_SVC = LinearSVC(C=1, penalty=\"l2\")\n",
      "                    Linear_SVC.fit(scaled_train_X, train_y_reduced)\n",
      "                    predicted_test_y = Linear_SVC.predict(scaled_test_X)\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new\n",
      "\n",
      "                    predicted_train_y = Linear_SVC.predict(scaled_train_X)\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))\n",
      "\n",
      "                    \n",
      "                if 1:\n",
      "                    print \"SVM_RBF\"\n",
      "                    (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, reduce_ratio = reduce_ratio)\n",
      "                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)\n",
      "                    scaled_train_X = standard_scaler.transform(train_X_reduced)\n",
      "                    scaled_test_X = standard_scaler.transform(test_X)\n",
      "                    L1_SVC_RBF_Selector = SVC(C=1, gamma=0.01, kernel='rbf').fit(scaled_train_X, train_y_reduced)\n",
      "\n",
      "                    predicted_test_y = L1_SVC_RBF_Selector.predict(scaled_test_X)\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM_RBF', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new\n",
      "\n",
      "                    predicted_train_y = L1_SVC_RBF_Selector.predict(scaled_train_X)\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM_RBF', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))\n",
      "          \n",
      "                # direct deep learning \n",
      "                min_max_scaler = Precessing_Scaler_0_9()\n",
      "                X_train_pre_validation_minmax = min_max_scaler.fit(train_X_reduced)\n",
      "                X_train_pre_validation_minmax = min_max_scaler.transform(train_X_reduced)\n",
      "                x_test_minmax = min_max_scaler.transform(test_X)\n",
      "                pretraining_X_minmax = min_max_scaler.transform(train_X_10fold)\n",
      "                x_train_minmax, x_validation_minmax, y_train_minmax, y_validation_minmax = train_test_split(X_train_pre_validation_minmax, \n",
      "                                                                                                  train_y_reduced\n",
      "                                                                    , test_size=0.4, random_state=42)\n",
      "                finetune_lr = 1\n",
      "                batch_size = 100\n",
      "                pretraining_epochs = cal_epochs(5000, x_train_minmax, batch_size = batch_size)\n",
      "                #pretrain_lr=0.001\n",
      "                pretrain_lr = 0.001\n",
      "                training_epochs = 1501\n",
      "                hidden_layers_sizes= [100, 100]\n",
      "                corruption_levels = [0,0]\n",
      "                if 1: \n",
      "                    # SAE_SVM\n",
      "                    print 'SAE followed by SVM'\n",
      "                    x = X_train_pre_validation_minmax\n",
      "                    a_MAE_A = train_a_MultipleAEs(x, pretraining_epochs=pretraining_epochs, pretrain_lr=pretrain_lr, batch_size=batch_size, \n",
      "                                            hidden_layers_sizes =hidden_layers_sizes, corruption_levels=corruption_levels)\n",
      "                    new_x_train_minmax_A =  a_MAE_A.transform(X_train_pre_validation_minmax)\n",
      "                    new_x_test_minmax_A =  a_MAE_A.transform(x_test_minmax)\n",
      "                    Linear_SVC = LinearSVC(C=1, penalty=\"l2\")\n",
      "                    Linear_SVC.fit(new_x_train_minmax_A, train_y_reduced)\n",
      "                    predicted_test_y = Linear_SVC.predict(new_x_test_minmax_A)\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SAE_SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new\n",
      "\n",
      "                    predicted_train_y = Linear_SVC.predict(new_x_train_minmax_A)\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SAE_SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))\n",
      "                 \n",
      "                                  \n",
      "                    \n",
      "                if 1:\n",
      "                    print \"direct deep learning\"\n",
      "                    sda = trainSda(x_train_minmax, y_train_minmax,\n",
      "                                 x_validation_minmax, y_validation_minmax , \n",
      "                                 x_test_minmax, test_y,\n",
      "                                 hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                                 training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                                 pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                     )\n",
      "                    print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                    print 'corruption_levels:', corruption_levels\n",
      "                    training_predicted = sda.predict(x_train_minmax)\n",
      "                    y_train = y_train_minmax\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'DL', isTest) + tuple(performance_score(y_train, training_predicted).values()))\n",
      "\n",
      "                    test_predicted = sda.predict(x_test_minmax)\n",
      "                    y_test = test_y\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'DL', isTest) + tuple(performance_score(y_test, test_predicted).values()))\n",
      "\n",
      "                if 0:\n",
      "                # deep learning using unlabeled data for pretraining\n",
      "                    print 'deep learning with unlabel data'\n",
      "                    pretraining_epochs = cal_epochs(5000, pretraining_X_minmax, batch_size = batch_size)\n",
      "                    sda_unlabel = trainSda(x_train_minmax, y_train_minmax,\n",
      "                                 x_validation_minmax, y_validation_minmax , \n",
      "                                 x_test_minmax, test_y, \n",
      "                                 pretraining_X_minmax = pretraining_X_minmax,\n",
      "                                 hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                                 training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                                 pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                     )\n",
      "                    print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                    print 'corruption_levels:', corruption_levels\n",
      "                    training_predicted = sda_unlabel.predict(x_train_minmax)\n",
      "                    y_train = y_train_minmax\n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'DL_U', isTest) + tuple(performance_score(y_train, training_predicted, predicted_score).values()))\n",
      "\n",
      "                    test_predicted = sda_unlabel.predict(x_test_minmax)\n",
      "                    y_test = test_y\n",
      "\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'DL_U', isTest) + tuple(performance_score(y_test, test_predicted, predicted_score).values()))\n",
      "                if 0:\n",
      "                    # deep learning using split network\n",
      "                    print 'deep learning using split network'\n",
      "                    # get the new representation for A set. first 784-D\n",
      "                    pretraining_epochs=5000\n",
      "                    hidden_layers_sizes =[100, 100, 100]\n",
      "                    corruption_levels=[0, 0, 0]\n",
      "                    \n",
      "                    x = x_train_minmax[:, :x_train_minmax.shape[1]/2]\n",
      "                    print \"original shape for A\", x.shape\n",
      "                    a_MAE_A = train_a_MultipleAEs(x, pretraining_epochs=pretraining_epochs, pretrain_lr=pretrain_lr, batch_size=batch_size, \n",
      "                                            hidden_layers_sizes =hidden_layers_sizes, corruption_levels=corruption_levels)\n",
      "                    new_x_train_minmax_A =  a_MAE_A.transform(x_train_minmax[:, :x_train_minmax.shape[1]/2])\n",
      "                    x = x_train_minmax[:, x_train_minmax.shape[1]/2:]\n",
      "                    \n",
      "                    print \"original shape for B\", x.shape\n",
      "                    a_MAE_B = train_a_MultipleAEs(x, pretraining_epochs=pretraining_epochs, pretrain_lr=pretrain_lr, batch_size=batch_size, \n",
      "                                            hidden_layers_sizes =hidden_layers_sizes, corruption_levels=corruption_levels)\n",
      "                    new_x_train_minmax_B =  a_MAE_B.transform(x_train_minmax[:, x_train_minmax.shape[1]/2:])\n",
      "                    \n",
      "                    new_x_test_minmax_A = a_MAE_A.transform(x_test_minmax[:, :x_test_minmax.shape[1]/2])\n",
      "                    new_x_test_minmax_B = a_MAE_B.transform(x_test_minmax[:, x_test_minmax.shape[1]/2:])\n",
      "                    new_x_validation_minmax_A = a_MAE_A.transform(x_validation_minmax[:, :x_validation_minmax.shape[1]/2])\n",
      "                    new_x_validation_minmax_B = a_MAE_B.transform(x_validation_minmax[:, x_validation_minmax.shape[1]/2:])\n",
      "                    new_x_train_minmax_whole = np.hstack((new_x_train_minmax_A, new_x_train_minmax_B))\n",
      "                    new_x_test_minmax_whole = np.hstack((new_x_test_minmax_A, new_x_test_minmax_B))\n",
      "                    new_x_validationt_minmax_whole = np.hstack((new_x_validation_minmax_A, new_x_validation_minmax_B))\n",
      "\n",
      "                    finetune_lr = 1\n",
      "                    batch_size = 100\n",
      "                    pretraining_epochs = cal_epochs(5000, x_train_minmax, batch_size = batch_size)\n",
      "                    #pretrain_lr=0.001\n",
      "                    pretrain_lr = 0.001\n",
      "                    training_epochs = 1500\n",
      "                    hidden_layers_sizes= [100, 100, 100]\n",
      "                    corruption_levels = [0,0,0]\n",
      "                    \n",
      "                    sda_transformed = trainSda(new_x_train_minmax_whole, y_train_minmax,\n",
      "                         new_x_validationt_minmax_whole, y_validation_minmax , \n",
      "                         new_x_test_minmax_whole, y_test,\n",
      "                         hidden_layers_sizes = hidden_layers_sizes, corruption_levels = corruption_levels, batch_size = batch_size , \\\n",
      "                         training_epochs = training_epochs, pretraining_epochs = pretraining_epochs, \n",
      "                         pretrain_lr = pretrain_lr, finetune_lr=finetune_lr\n",
      "                         )\n",
      "                    \n",
      "                    print 'hidden_layers_sizes:', hidden_layers_sizes\n",
      "                    print 'corruption_levels:', corruption_levels\n",
      "                    training_predicted = sda_transformed.predict(new_x_train_minmax_whole)\n",
      "                    y_train = y_train_minmax\n",
      "                    \n",
      "                    isTest = False; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'DL_S', isTest) + tuple(performance_score(y_train, training_predicted, predicted_score).values()))\n",
      "\n",
      "                    test_predicted = sda_transformed.predict(new_x_test_minmax_whole)\n",
      "                    y_test = test_y\n",
      "\n",
      "                    isTest = True; #new\n",
      "                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'DL_S', isTest) + tuple(performance_score(y_test, test_predicted, predicted_score).values()))\n",
      "            \n",
      "            \n",
      "            report_name = filename + '_' + '_test10fold_'.join(map(str, hidden_layers_sizes)) + \\\n",
      "                            '_' + str(pretrain_lr) + '_' + str(finetune_lr) + '_' + str(reduce_ratio)+ \\\n",
      "                    '_' + str(training_epochs) + '_' + current_date\n",
      "            saveAsCsv(predicted_score, report_name, performance_score(y_test, test_predicted, predicted_score), analysis_scr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ten_fold_crossvalid_performance_for_all(ddis)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:__main__:DDI: 6PGD_int_NAD_binding_2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Subset:', 1)\n",
        "('Train index: ', array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
        "       23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
        "       40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]))\n",
        "('Test index: ', array([0, 1, 2, 3, 4, 5]))\n",
        "SVM\n",
        "recall"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 56.1%\n",
        "precision 52.7%\n",
        "accuracy 52.9%\n",
        "recall 60.2%\n",
        "precision 54.3%\n",
        "accuracy 54.8%\n",
        "\n",
        "\n",
        "\n",
        "direct deep learning\n",
        "... building the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... getting the pretraining functions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... pre-training the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pre-training layer 0, epoch 0, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 58.4449638819\n",
        "Pre-training layer 0, epoch 1, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 51.6941089819\n",
        "Pre-training layer 0, epoch 2, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46.1313883683\n",
        "Pre-training layer 0, epoch 3, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 41.6093603624\n",
        "Pre-training layer 0, epoch 4, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 37.9779297601\n",
        "Pre-training layer 0, epoch 5, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 35.0915477392\n",
        "Pre-training layer 0, epoch 6, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 32.8165703904\n",
        "Pre-training layer 0, epoch 7, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 31.0358784105\n",
        "Pre-training layer 0, epoch 8, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 29.650016514\n",
        "Pre-training layer 0, epoch 9, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 28.5762322058\n",
        "Pre-training layer 0, epoch 10, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 27.7467543475\n",
        "Pre-training layer 0, epoch 11, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 27.1069087275\n",
        "Pre-training layer 0, epoch 12, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26.6132308636\n",
        "Pre-training layer 0, epoch 13, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26.2316339302\n",
        "Pre-training layer 0, epoch 14, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.9356954694\n",
        "Pre-training layer 0, epoch 15, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.7051234087\n",
        "Pre-training layer 0, epoch 16, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.5244386743\n",
        "Pre-training layer 0, epoch 17, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.3818842\n",
        "Pre-training layer 0, epoch 18, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.2685483487\n",
        "Pre-training layer 0, epoch 19, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.1776774193\n",
        "Pre-training layer 0, epoch 20, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.1041462706\n",
        "Pre-training layer 0, epoch 21, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25.0440560243\n",
        "Pre-training layer 0, epoch 22, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.994430982\n",
        "Pre-training layer 0, epoch 23, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.9529914343\n",
        "Pre-training layer 0, epoch 24, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.9179837282\n",
        "Pre-training layer 0, epoch 25, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.8880531616\n",
        "Pre-training layer 0, epoch 26, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.8621487633\n",
        "Pre-training layer 0, epoch 27, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.8394517665\n",
        "Pre-training layer 0, epoch 28, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.819321693\n",
        "Pre-training layer 0, epoch 29, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.8012555528\n",
        "Pre-training layer 0, epoch 30, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.7848568359\n",
        "Pre-training layer 0, epoch 31, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.769811843\n",
        "Pre-training layer 0, epoch 32, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.7558715379\n",
        "Pre-training layer 0, epoch 33, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.7428375726\n",
        "Pre-training layer 0, epoch 34, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.7305514787\n",
        "Pre-training layer 0, epoch 35, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.7188862733\n",
        "Pre-training layer 0, epoch 36, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.7077399141\n",
        "Pre-training layer 0, epoch 37, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6970301761\n",
        "Pre-training layer 0, epoch 38, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.686690627\n",
        "Pre-training layer 0, epoch 39, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6766674544\n",
        "Pre-training layer 0, epoch 40, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6669169562\n",
        "Pre-training layer 0, epoch 41, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6574035503\n",
        "Pre-training layer 0, epoch 42, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6480981899\n",
        "Pre-training layer 0, epoch 43, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6389770997\n",
        "Pre-training layer 0, epoch 44, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6300207653\n",
        "Pre-training layer 0, epoch 45, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6212131219\n",
        "Pre-training layer 0, epoch 46, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6125409032\n",
        "Pre-training layer 0, epoch 47, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.6039931166\n",
        "Pre-training layer 0, epoch 48, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5955606196\n",
        "Pre-training layer 0, epoch 49, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5872357778\n",
        "Pre-training layer 0, epoch 50, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5790121878\n",
        "Pre-training layer 0, epoch 51, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5708844519\n",
        "Pre-training layer 0, epoch 52, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5628479962\n",
        "Pre-training layer 0, epoch 53, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5548989214\n",
        "Pre-training layer 0, epoch 54, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5470338825\n",
        "Pre-training layer 0, epoch 55, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5392499898\n",
        "Pre-training layer 0, epoch 56, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24.5315447287\n",
        "Pre-training layer 0, epoch 57, cost "
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#LOO_out_performance_for_all(ddis)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = logging._handlers.copy()\n",
      "for i in x:\n",
      "    log.removeHandler(i)\n",
      "    i.flush()\n",
      "    i.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}